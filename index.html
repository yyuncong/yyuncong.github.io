<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuncong Yang</title>
  
  <meta name="author" content="Yuncong Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuncong Yang</name>
              </p>
              <p>I am currently a first-year PhD student at UMass Amherst, where I am supervised by Prof.  <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a> and collaborate with Prof.  <a href="https://yilundu.github.io/">Yilun Du</a>.
                Previously, I graduated from Columbia's Fu Foundation School of Engineering with both an M.S. and a B.S. in Computer Science <em>(Summa Cum Laude)</em>.
                I was fortunate to work under the supervision of Prof. <a href="https://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a> at Columbia University and with Dr. <a href="https://jimfan.me/">Jim Fan</a>, Prof. <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, and Prof. <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a> at NVIDIA Research.
             </p>
              <p>
                My research interests lie in the area of Embodied AI and Multi-modal Foundation Models.
              </p>
              <p style="text-align:center">
                <a href="data/cv_yuncong.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=IgQuUY0AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/yyupsong">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/yyuncong/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yuncong.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuncong_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <li> [2023.01] One paper was accepted by <a href="https://iclr.cc/Conferences/2023" target="_blank">ICLR 2023</a>.
              <li> [2022.11] <a href="https://minedojo.org/">MineDojo</a> has won <strong>Outstanding Paper Award</strong> at <a href="https://neurips.cc/virtual/2022/awards_detail">NeurIPS 2022</a>!</li>
              <li> [2022.07] One paper was accepted by <a href="https://eccv2022.ecva.net/" target="_blank">ECCV 2022</a>.
              <li> [2022.06] Officially released <a href="https://minedojo.org/">MineDojo</a>!</li>
              <li> [2022.01] Join the Nvidia AI research group, supervised by Dr. <a href="https://jimfan.me/">Jim Fan</a>, Prof. <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>, and Prof. <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>.</li>
              <li> [2021.09] Join the <a href="https://www.ee.columbia.edu/ln/dvmm/">Digital Video and Multimedia (DVMM) Lab</a> at Columbia University, supervised by Prof. <a href="https://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a></li>
              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <heading>Research</heading>
              <p>
              (* indicates equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <div class="one">
                <video height="135" muted autoplay loop>
                <source src="images/snapmem_video_intro.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video>
              </div>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <a href="https://vis-www.cs.umass.edu/snapmem/">
                <papertitle>SnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning
                </papertitle>
              </a>
              <br>
              <strong>Yuncong Yang*</strong>,
              <a href="https://hanyangclarence.github.io/">Han Yang*</a>,
              <a href="https://www.linkedin.com/in/jiachen-zhou5/">Jiachen Zhou</a>,
              <a href="https://peihaochen.github.io/">Peihao Chen</a>,
              <a href="https://icefoxzhx.github.io/">Hongxin Zhang</a>,
              <a href="https://yilundu.github.io/">Yilun Du</a>,
              <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
              <br>
                <em>Under Review</em>
              <br>
              <a href="https://vis-www.cs.umass.edu/snapmem/">project page</a>
              /
              paper (coming soon)
              /
              code (coming soon)
              <p></p>
              <p>
                We proposed SnapMem, a snapshot-based scene memory that empowers embodied agents with lifelong exploration and reasoning abilities in 3D environments.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:30%;vertical-align:middle">
              <img src='images/tempclr.png' height="135">
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.13738">
                <papertitle>TempCLR: Temporal Alignment Representation with Contrastive Learning
                </papertitle>
              </a>
              <br>
              <strong>Yuncong Yang*</strong>,
              <a href="https://blogs.cuit.columbia.edu/jm4743/">Jiawei Ma*</a>,
              <a href="https://shiyuanh.github.io/">Shiyuan Huang</a>,
              <a href="https://zjuchenlong.github.io/">Long Chen</a>,
              <a href="https://xudonglinthu.github.io/">Xudong Lin</a>,
              <a href="https://guangxinghan.github.io/">Guangxing Han</a>,
              <a href="https://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a>
              <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2212.13738">paper</a>
              /
              <a href="https://github.com/yyuncong/TempCLR">code</a>
              <p></p>
              <p>
                We proposed TempCLR, a new contrastive learning framework that considers sequence-level temporal order consistency in Long-Video Understanding.
              </p>
            </td>
          </tr>

          <tr onmouseout="minedojo_stop()" onmouseover="minedojo_start()">
            <td style="padding:10px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='minedojo_image'><video height="135" muted autoplay loop>
                <source src="images/minedojo.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/minedojo.jpg' height="135">
              </div>
              <script type="text/javascript">
                function minedojo_start() {
                  document.getElementById('minedojo_image').style.opacity = "1";
                }

                function minedojo_stop() {
                  document.getElementById('minedojo_image').style.opacity = "0";
                }
                minedojo_stop()
              </script>
            </td>
            <td style="padding:10px;width:70%;vertical-align:middle">
              <a href="https://minedojo.org/">
                <papertitle>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge
                </papertitle>
              </a>
              <br>
              <a href="https://jimfan.me/">Linxi Fan</a>,
              <a href="https://guanzhi.me/">Guanzhi Wang*</a>,
              <a href="https://yunfanj.com/">Yunfan Jiang*</a>,
              <a href="https://research.nvidia.com/person/ajay-mandlekar">Ajay Mandlekar</a>,
              <strong>Yuncong Yang</strong>,
							<a href="https://www.haoyizhu.site/">Haoyi Zhu</a>,
              <a href="https://twitter.com/andrewtang01">Andrew Tang</a>,
              <a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,
              <a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a>,
              <a href="http://tensorlab.cms.caltech.edu/users/anima/">Animashree Anandkumar</a>
              <br>
              <em>Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track</em>, 2022 &nbsp <font color="red"><strong>(Outstanding Paper Award, Featured Paper Presentation)</strong></font>
              <br>
              <a href="https://minedojo.org/">project page</a>
              /
              <a href="https://arxiv.org/abs/2206.08853">paper</a>
              /
              <a href="https://github.com/MineDojo/MineDojo">code</a>
              <p></p>
              <p>
                We introduce MineDojo, a new framework based on the popular Minecraft game for building generally capable, open-ended embodied agents.
              </p>
            </td>
          </tr>

					<tr>
            <td style="padding:10px;width:30%;max-width:30%;vertical-align:middle">
              <img src='images/fewshot.png' width="240" height="135">
            </td>
            <td style="padding:10px;width:70%;max-width:70%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860056.pdf">
                <papertitle>Few-Shot End-to-End Object Detection via Constantly Concentrated Encoding across Heads
                </papertitle>
              </a>
              <br>
              <a href="https://blogs.cuit.columbia.edu/jm4743/">Jiawei Ma</a>,
              <a href="https://guangxinghan.github.io/">Guangxing Han</a>,
              <a href="https://shiyuanh.github.io/">Shiyuan Huang</a>,
              <strong>Yuncong Yang</strong>,
              <a href="https://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a>
              <br>
                <em>European Conference on Computer Vision (ECCV)</em>, 2022
              <br>
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860056.pdf">paper</a>
              /
              code comming soon
              <p></p>
              <p>
              </p>
            </td>
          </tr>
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:10px;width:30%;vertical-align:middle">
          <heading>Misc Projects</heading>
        </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/robustness.png' height="135">
          </td>
          <td style="padding:10px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2212.13738">
              <papertitle>Adversarial Training for Few-Shot Image Classifications
              </papertitle>
            </a>
            <br>
              <em>COMS 6998 Security Robustness ML Systems</em>, Fall 2021
            <br>
            <a href="https://drive.google.com/file/d/1kYQh66SPLdXhqIvAlFZf5AZ3WGPbWRqR/view?usp=sharing">paper</a>
            <p></p>
          </td>
        </tr>

        <tr>
          <td style="padding:10px;width:30%;vertical-align:middle">
            <img src='images/dynamic_grasping.png' height="135">
          </td>
          <td style="padding:10px;width:70%;vertical-align:middle">
            <a href="https://drive.google.com/file/d/16TtCIzB4km3THUjVWXXqNaSDzWnSSjmy/view?usp=sharing">
              <papertitle>Dynamic Grasping with Moving Obstacles
              </papertitle>
            </a>
            <br>
              <em>COMS 6998 Topics in Robot Learning</em>, Fall 2021
            <br>
            <a href="https://drive.google.com/file/d/16TtCIzB4km3THUjVWXXqNaSDzWnSSjmy/view?usp=sharing">report</a>
            <p></p>
          </td>
        </tr>
      </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr>
        <td style="padding:10px;width:30%;vertical-align:middle">
          <heading>Teaching</heading>
        </td>
      </tr>
      </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <td style="padding:10px;width:100%;vertical-align:middle">
            Teaching Assistant: COMS 4732 Computer Vision II (Spring 2022)
            Teaching Assistant: CS250 Intro to Computation (Fall 2024)
        </td>
      </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks for the template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>!
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      
      </td>
    </tr>
  </table>
</body>

</html>
